# -*- coding: utf-8 -*-
"""A01745914_Momento de Retroalimentación: Módulo 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CdgIWeBa-Bt3EYXqsLGeSyDB5-0_xS0t?usp=sharing

#Momento de Retroalimentación - Módulo 2 - Correcciones
##Naomi Padilla Mora A01745914

#Librerías

Importamos las librerias necesarias
"""

import pandas as pd
import numpy as np
from pandas import read_csv
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

"""#Base de datos

Importamos la base de datos
Para esta entrega se realizó un cambio de dataset ya que el anterior estaba muy pequeño y no permitía generar un árbol con mayor profundidad y mejores métricas

"""

#colocar path del csv. Click derecho, copy path
df = pd.read_csv("/Users/naomipadilla/Downloads/A01745914_Momento de Retro-Mod2-Correcciones/Automobile.csv", header=0)
df.head()

"""#Limpieza del Dataset

##Información del dataset

Información básica sobre la base de datos.

*   26 columnas y 205 filas
*   Inicialmente contamos con 16 variables categóricas (object), 5 variables numéricas (5 int y 5 float)
"""

df.shape

#Tipos de variables
df.dtypes

"""##Diccionario

Observando el tipo de variables del dataset, tenemos lo siguiente:

| Variable | Tipo | Categoría |
|----------|----------|----------|
| symboling    | Int   | Numérica Discreta   |
| normalized-losses    | Int   | Numérica Discreta    |
| make   | Object   | Categórica Nominal    |
| fuel-type   | Object   | Categórica Nominal    |
| aspiration    | Object   | Categórica Nominal    |
| number-of-doors    | Int   | Numérica Discreta    |
| body-style    | Object   | Categórica Nominal    |
| drive-wheels    | Object   | Categórica Nominal    |
| engine-location    | Object   | Categórica Nominal    |
| wheel-base   | Float   | Numérica Continua    |
| lenght    | Float   | Numérica Continua    |
| wigth    | Float   | Numérica Continua    |
| height    | Float   | Numérica Continua    |
| curb-weight    | Int   | Numérica Continua    |
| engine-type    | Object   | Categórica Nominal    |
| num-of-cylinders    | Int   | Numérica Discreta    |
| engine-size   | Int   | Numérica Discreta    |
| fuel-system    | Object   | Categórica Nominal    |
| bore    | Float   | Numérica Continua   |
| stroke    | Float   | Numérica Continua    |
| compression-ratio    | Float   | Numérica Continua    |
| horsepower    | Int   | Numérica Discreta    |
| peak-rpm    | Int   | Numérica Discreta    |
| city-mpg    | Int   | Numérica Discreta    |
| highway-mpg    | Int   | Numérica Discreta    |
| price   | Int   | Numérico Discreta   |

"""

#Conversión de las variables numéricas continuas a numéricas discretas(int).
df['normalized-losses'] = df['normalized-losses'].astype('Int64')
df['number-of-doors'] = df['number-of-doors'].astype('Int64')
df['num-of-cylinders'] = df['num-of-cylinders'].astype('Int64')
df['horsepower'] = df['horsepower'].astype('Int64')
df['peak-rpm'] = df['peak-rpm'].astype('Int64')
df['price'] = df['price'].astype('Int64')

df.head()

#comprobamos que tenemos el tipo de variables deseado
df.dtypes

"""Posteriormente a este procesamiento, ahora tenemos la clasificación correcta de las variables como se puede comprobar con el comando **dtypes**.

##Valores duplicados
Con el comando duplicated se observará si hay registros duplicados en el dataset.
"""

#comando para saber su hay alguna entrada duplicada en nuestro dataset
df.duplicated(subset=None)

"""Obteniendo el valor **False** para cada renglón, podemos confirmar que no hay ningún registro duplicado en el dataset, por lo que ningún procesamiento será necesario para esto.

##Valores faltantes o nulos
"""

#Comando para ver la cantidad de valores nulos en cada variable.
df.isna().sum()

"""Tras este análisis, con el comando **isna**, podemos destacar lo siguiente:

Estas variables presentan como valor **?**, implicando que este es un Non Value, para posteriormente procesarlos de la manera correspondiente.

| Variable | Tipo | #? - NaN |
|----------|----------|----------|
| normalized-losses    | Int   | 41  |
| number-of-doors    | Int   | 2   |
| bore    | Float   | 4   |
| stroke    | Float   | 4  |
| horsepower    | Int   | 2  |
| peak-rpm    | Int   | 2  |
| price   | Int   | 4  |

##Tratamiento de los NaN values.

Como se observó en el apartado de Valores faltantes o nulos, las siguientes variables presentan esta clase de datos y en este apartado se analizarán y se procesaran de la forma correspondiente.

| Variable | Tipo | #? - NaN |
|----------|----------|----------|
| normalized-losses    | Int   | 41  |
| number-of-doors    | Int   | 2   |
| bore    | Float   | 4   |
| stroke    | Float   | 4  |
| horsepower    | Int   | 2  |
| peak-rpm    | Int   | 2  |
| price   | Int   | 4  |

Debido a las dimensiones del dataset, 205 filas y 26 columnas, no resulta conveniente solo eliminar los valores nulos, ya que, eliminar las entradas con estos valores, quitaría un mínimo de 41 entradas de todo el dataset, esto último, podría afectar negativamente a un análisis futuro o implementación de algún modelo.

Por ello, la técnica de procesamiento será reemplazar los valores nulos. Se realizarán los histogramas de cada una de estas variables con el objetivo de ver su distribución y poder decidir cuál sería la mejor forma de reemplazar estos valores (media, mediana).
"""

grid = plt.GridSpec(3, 3, wspace=0.7, hspace=0.6)
plt.subplot(grid[0, 0])
sns.histplot(data=df, x=df["normalized-losses"], bins= 'auto', color = 'blue')
#plt.ylabel('F')
plt.subplot(grid[0, 1])
sns.histplot(data=df, x=df["number-of-doors"], bins= 'auto', color = 'blue')
#plt.ylabel('F')
plt.subplot(grid[0, 2])
sns.histplot(data=df, x=df["bore"], bins= 'auto', color = 'blue')
#plt.ylabel('F')
plt.subplot(grid[1, 0])
sns.histplot(data=df, x=df["stroke"], bins= 'auto', color = 'blue')
#plt.ylabel('F')
plt.subplot(grid[1, 1])
sns.histplot(data=df, x=df["horsepower"], bins= 'auto', color = 'blue')
#plt.ylabel('F')
plt.subplot(grid[1, 2])
sns.histplot(data=df, x=df["peak-rpm"], bins= 'auto', color = 'blue')
#plt.ylabel('F')
plt.subplot(grid[2, :])
sns.histplot(data=df, x=df["price"], bins= 'auto', color = 'blue')
#plt.ylabel('F')

"""De las variables anteriores, solo **bore** y **peak-rpm** muestran una distribución normal. Por lo que sus valores faltantes se reemplazarán por la **media**. Mientras que para el resto de variables (**normalized-losses**, **number-of-doors**, **stroke**, **horsepower** y **price**) que al presentar un sesgo, a la izquierda o derecha, la mejor forma de reemplazar sus valores nulos, es con la **mediana** de los datos. Esto ya que, al tener un sesgo en la distribución, su media y mediana no son iguales, y por lo tanto, la concentración de los datos no se encuentra al centro."""

#media
df['bore']=df['bore'].fillna(df['bore'].median())
df['peak-rpm']=df['peak-rpm'].fillna(df['peak-rpm'].median())

#mediana
df['normalized-losses']=df['normalized-losses'].fillna(df['normalized-losses'].median())
df['number-of-doors']=df['number-of-doors'].fillna(df['number-of-doors'].median())
df['stroke']=df['stroke'].fillna(df['stroke'].median())
df['horsepower']=df['horsepower'].fillna(df['horsepower'].median())
df['price']=df['price'].fillna(df['price'].median())

df.isna().sum().sum()

"""Tras utilizar nuevamente el comando isna podemos confirmar que ya no hay ningún valor nulo en nuestro dataset por lo que se encuentra listo para futuro análisis o implementación de un modelo.

##Variables numéricas

Separamos las varianles numéricas de las variables categóricas
"""

#dataframe de solo las variables numéricas
df_num = df.select_dtypes(include=["number"])

#array con los headers de las variables numéricas
df_columns_num = df_num.columns.to_numpy()
df_columns_num

"""##Variables categóricas

Transformación de las variables categóricas a numéricas usando la función dummy.

"""

#dataframe de solo las variables categóricas
df_obj = df.select_dtypes(include=["object"])

#array con los headers de las variables categóricas
df_columns_obj = df_obj.columns.to_numpy()
df_columns_obj

dummies = [df]

for columna in df_columns_obj:
    df_codificado = pd.get_dummies(df[columna], prefix=columna)
    dummies.append(df_codificado)

# Combinar los DataFrames codificados en uno solo
df = pd.concat(dummies, axis=1)
df.drop(df_columns_obj, axis=1, inplace=True)

df.shape

df_colums = df.columns.to_numpy()
df_colums

"""##Correlación"""

numericas = df.select_dtypes(include=['int64','float64'])

sns.set(rc = {'figure.figsize':(80,60)})
sns.heatmap(numericas.corr(), annot=True)

"""#Modelo

Implementación de un modelo de árbol de decisión sin el uso de librerías

##Separación de los datos en train y test
Para este entregable no fue posible hacer la division en train, test y val, ya que no se logró adaptar el modelo de manera exitosa.
"""

#definimos las variables que tendremos en X y en y (variable a predecir)
X = df.drop(columns=["price"]) #eliminamos la columna price
y = pd.DataFrame(df["price"]) #mantenemos solo la columna price

#haciendo uso del train_test_split con un 30% para test y 70% para train, separamos nuestro df
train_data, test_data = train_test_split(df, test_size=0.3, random_state=42)
#damos una semilla aleatoria, esta se puede cambiar pero alteraría los resultados del modelo en cada ocasión.

X_train = X.values
y_train = y.values
X_test = X.values
y_test = y.values

#ya que al realizar el train_test_split nuestras variables y_train y y_test se vuelven de tipo object las transformaremos a int64
# Convierte 'y_test' a int64
y_test = y_test.astype('int64')

# Verifica el nuevo tipo de 'y_test'
print(y_test.dtype)

# Convierte 'y_train' a int64
y_train = y_train.astype('int64')

# Verifica el nuevo tipo de 'y_train'
print(y_train.dtype)

"""##Árbol de decisión"""

#solo correr
#función para calcular el índice de gini que mide la impureza de los datos según sus variables.
#A mayor índicede de Gini, mayor será la impureza
def calcular_gini(labels):
    total = len(labels)
    conteo_clases = {} #diccionario para contar cuantas veces aparece la etiqueta
    #labels es el arreglo que contiene las variables de df - clases.

    #este ciclo recorre todo el arreglo y cuenta cuántas veces aparece cada en el diccionario
    for label in labels:
        if label not in conteo_clases:
            conteo_clases[label] = 0
        conteo_clases[label] += 1

    impureza_gini = 1.0 #definimos la impureza

    #calcula la probabilidad de que una instancia elegida al azar pertenezca a la clase aplicando la operación siguiente
    for clase, conteo in conteo_clases.items():
        probabilidad = conteo / total
        impureza_gini -= probabilidad ** 2

    return impureza_gini

#solo correr
#esta función divide el conjunto de datos de tal forma que los de la izquierda son menores al valor y los de la derecha mayores
# esto con el obj de dividir en subconjuntos mas puros al cumplir una característica, facilitando la decisión en cada nodo
def dividir_datos(data, columna, valor):
    izquierda, derecha = [], []
    for fila in data:
        if fila[columna] <= valor:
            izquierda.append(fila) #si la condición se cumple se van a la izquierda
        else:
            derecha.append(fila) #si la condición no se cumple de van a la derecha
    return izquierda, derecha

#solo correr
#construcción del arbol, recibe el df y la profundidad deseada del árbol, genera un árbol usando el critrio del índice de gini
def construir_arbol(data, profundidad_maxima):
    if profundidad_maxima == 0: #inicializamos en 0 porque esta se generalizará. Una vez se alcanza esta profundidad el código se detiene
        etiquetas = [fila[-1] for fila in data]
        return max(etiquetas, key=etiquetas.count)

    mejor_gini = float('inf') #ini val
    mejor_division = None #ini val

   #se itera en todo el df para hacer la división de los datos usando la función dividir_datos
    for columna in range(len(data[0]) - 1):
        for fila in data:
            izquierda, derecha = dividir_datos(data, columna, fila[columna])

            if not izquierda or not derecha:
                continue


            gini_total = (len(izquierda) / len(data)) * calcular_gini([fila[-1] for fila in izquierda]) + \
                         (len(derecha) / len(data)) * calcular_gini([fila[-1] for fila in derecha])

            #toma el mejor valor de Gini, el más bajo para garantizar la impureza
            if gini_total < mejor_gini:
                mejor_gini = gini_total
                mejor_division = (columna, fila[columna])

    if mejor_gini == float('inf'):
        etiquetas = [fila[-1] for fila in data]
        return max(etiquetas, key=etiquetas.count)

    izquierda, derecha = dividir_datos(data, *mejor_division)
    nodo = {'columna': mejor_division[0], 'valor': mejor_division[1]} #diccionario nodo, nodo actual para la comnstruccón del árbopl en izq y derecha
    #cada nodo es una división de los datos
    nodo['izquierda'] = construir_arbol(izquierda, profundidad_maxima - 1)
    nodo['derecha'] = construir_arbol(derecha, profundidad_maxima - 1)
    return nodo

#solo correr
#esta función utiliza al arbol previemente generado para obtener la predicción de la entrada deseada
def clasificar_ejemplo(ejemplo, arbol):
    if isinstance(arbol, int):
        return arbol  # Devuelve el valor de etiqueta en un nodo hoja

    columna, valor = arbol['columna'], arbol['valor']
    if ejemplo[columna] <= valor:
        return clasificar_ejemplo(ejemplo, arbol['izquierda'])
    else:
        return clasificar_ejemplo(ejemplo, arbol['derecha'])

"""##Metricas de evaluación del modelo"""

#solo correr
def metricas_modelo(y_test, predicciones):
    # Precisión (Accuracy)
    accuracy = accuracy_score(y_test, predicciones)
    print("Accuracy:", accuracy)

    # Precisión (Precision)
    precision = precision_score(y_test, predicciones, average='weighted')
    print("Precision:", precision)

    # Exhaustividad (Recall)
    recall = recall_score(y_test, predicciones, average='weighted')
    print("Recall:", recall)

    # Puntaje F1 (F1-score)
    f1 = f1_score(y_test, predicciones, average='weighted')
    print("F1-score:", f1)

    # Matriz de confusión
    confusion = confusion_matrix(y_test, predicciones)
    print("Matriz de Confusión:")
    print(confusion)

"""#Prueba del modelo"""

# Construir el árbol de decisión
arbol = construir_arbol(np.column_stack((X_train, y_train)), profundidad_maxima=10) #profundidad de 10

# Realizar predicciones en el conjunto de prueba
predicciones = [clasificar_ejemplo(ejemplo, arbol) for ejemplo in X_test]

metricas = metricas_modelo(y_test, predicciones)

"""#Generalización del modelo

La generalización del modelo consiste en dos partes.

1. La creación del árbol y su evaluación.

2. El cálculo de las predicciones según las entradas.

Una entrada se conforma de todas las siguientes variables:

array(['symboling', 'normalized-losses', 'number-of-doors', 'wheel-base',
       'length', 'width', 'height', 'curb-weight', 'num-of-cylinders',
       'engine-size', 'bore', 'stroke', 'compression-ratio', 'horsepower',
       'peak-rpm', 'city-mpg', 'highway-mpg', 'price', 'make_alfa-romero',
       'make_audi', 'make_bmw', 'make_chevrolet', 'make_dodge',
       'make_honda', 'make_isuzu', 'make_jaguar', 'make_mazda',
       'make_mercedes-benz', 'make_mercury', 'make_mitsubishi',
       'make_nissan', 'make_peugot', 'make_plymouth', 'make_porsche',
       'make_renault', 'make_saab', 'make_subaru', 'make_toyota',
       'make_volkswagen', 'make_volvo', 'fuel-type_diesel',
       'fuel-type_gas', 'aspiration_std', 'aspiration_turbo',
       'body-style_convertible', 'body-style_hardtop',
       'body-style_hatchback', 'body-style_sedan', 'body-style_wagon',
       'drive-wheels_4wd', 'drive-wheels_fwd', 'drive-wheels_rwd',
       'engine-location_front', 'engine-location_rear',
       'engine-type_dohc', 'engine-type_dohcv', 'engine-type_l',
       'engine-type_ohc', 'engine-type_ohcf', 'engine-type_ohcv',
       'engine-type_rotor', 'fuel-system_1bbl', 'fuel-system_2bbl',
       'fuel-system_4bbl', 'fuel-system_idi', 'fuel-system_mfi',
       'fuel-system_mpfi', 'fuel-system_spdi', 'fuel-system_spfi'])

Se pueden utilizar los valores deseados para estas pero para fines de practicidad de tomaran entradas específicas del df de test.

##Árbol profundidad_máxima=10

Como se observó, el árbol con una profundidad máxima no tiene buenos puntajes en las métricas por lo que se esperan predicciones alejadas de la realidad.
"""

metricas = metricas_modelo(y_test, predicciones)

"""###Caso 1

Evaluación del modelo con una profundidad máxima de 10, caso de ejemplo la entrada 26 del df.

Y el resultado esperado para esta entrada es price = 7609


"""

X_test[26]

y_test[26]

# Cálculo de las predicciones según las entradas deseadas y el arbol antes generado
# Ejemplo de clasificación
# Se pueden cambiar a los valores deseados
ejemplo = X_test[26] #array con todas las variables de una entrada
resultado = clasificar_ejemplo(ejemplo, arbol)
print("Resultado de clasificación:", resultado)

"""La predicción no fue acertada.

###Caso 2

Evaluación del modelo con una profundidad máxima de 10, caso de ejemplo la entrada 1 del df.

Y el resultado esperado para esta entrada es price = 16500
"""

X_test[1]

y_test[1]

# Cálculo de las predicciones según las entradas deseadas y el arbol antes generado
# Ejemplo de clasificación
# Se pueden cambiar a los valores deseados
ejemplo = X_test[1] #array con todas las variables de una entrada
resultado = clasificar_ejemplo(ejemplo, arbol)
print("Resultado de clasificación:", resultado)

"""La predicción no fue acertada.

##Árbol profundidad_máxima=30

Tomando en cuenta los resultados anteriores y que a mayor profundidad mejor será la precisión del modelo, tenemos lo siguiente:
"""

# Creación del árbol y evaluación
arbol = construir_arbol(np.column_stack((X_train, y_train)), profundidad_maxima=30) #profundidad de 30

# Realizar predicciones en el conjunto de prueba
predicciones = [clasificar_ejemplo(ejemplo, arbol) for ejemplo in X_test]

metricas = metricas_modelo(y_test, predicciones)

"""Como se observa en las métricas, la profundidad ayudó bastante a mejorar la precisión del modelo y sus métricas. De lo cual podemos decir que al ser mayor de 0.95 tenemos un buen modelo.

###Caso 1

Evaluación del modelo con una profundidad máxima de 30, caso de ejemplo la entrada 0 del df.

Y el resultado esperado para esta entrada es price = 13495
"""

X_test[0]

y_test[0]

# Cálculo de las predicciones según las entradas deseadas y el arbol antes generado
# Ejemplo de clasificación
# Se pueden cambiar a los valores deseados
ejemplo = X_test[0] #array con todas las variables de una entrada
resultado = clasificar_ejemplo(ejemplo, arbol)
print("Resultado de clasificación:", resultado)

"""###Caso 2

Evaluación del modelo con una profundidad máxima de 30, caso de ejemplo la entrada 15 del df.


Y el resultado esperado para esta entrada es price = 30760
"""

X_test[15]

y_test[15]

# Cálculo de las predicciones según las entradas deseadas y el arbol antes generado
# Ejemplo de clasificación
# Se pueden cambiar a los valores deseados
ejemplo = X_test[15] #array con todas las variables de una entrada
resultado = clasificar_ejemplo(ejemplo, arbol)
print("Resultado de clasificación:", resultado)

"""El modelo ya muestra una excelente precisión en el cálculo de la predicción, sin embargo, se creará un árbol con una profundidad más alta solo para ver si aumentan sus métricas.

##Árbol profundidad_máxima=50
"""

# Creación del árbol y evaluación
arbol = construir_arbol(np.column_stack((X_train, y_train)), profundidad_maxima=50) #profundidad de 50

# Realizar predicciones en el conjunto de prueba
predicciones = [clasificar_ejemplo(ejemplo, arbol) for ejemplo in X_test]

metricas = metricas_modelo(y_test, predicciones)

"""Como podemos ver, las métricas se mantienen, por lo que ya no es necesario aumentar la profundidad del parbol.

###Caso 1

Evaluación del modelo con una profundidad máxima de 50, caso de ejemplo la entrada 105 del df.

Y el resultado esperado para esta entrada es price = 19699
"""

X_test[105]

y_test[105]

# Cálculo de las predicciones según las entradas deseadas y el arbol antes generado
# Ejemplo de clasificación
# Se pueden cambiar a los valores deseados
ejemplo = X_test[105] #array con todas las variables de una entrada
resultado = clasificar_ejemplo(ejemplo, arbol)
print("Resultado de clasificación:", resultado)

"""###Caso 2

Evaluación del modelo con una profundidad máxima de 50, caso de ejemplo la entrada 177 del df.

Y el resultado esperado para esta entrada es price = 11248
"""

X_test[177]

y_test[177]

# Cálculo de las predicciones según las entradas deseadas y el arbol antes generado
# Ejemplo de clasificación
# Se pueden cambiar a los valores deseados
ejemplo = X_test[177] #array con todas las variables de una entrada
resultado = clasificar_ejemplo(ejemplo, arbol)
print("Resultado de clasificación:", resultado)

"""Como podemos comprobar, el modelo funciona de manera apropiada y es posible generalizarlo a la profundidad deseada y con los valores de ejemplo deseados. Para mayor practicidad se recomienda usar uno de los valores de df de X_test, usando la instraucción X_test[i] donde i = 0 a 205.

#Finalmente

Para probar el modelo solo es necesario realizar lo siguiente:

1. La creación del árbol y su evaluación.

2. El cálculo de las predicciones según las entradas.
"""

# Creación del árbol y evaluación
arbol = construir_arbol(np.column_stack((X_train, y_train)), profundidad_maxima=0) #colocar profundidad deseada

# Realizar predicciones en el conjunto de prueba
predicciones = [clasificar_ejemplo(ejemplo, arbol) for ejemplo in X_test]

metricas = metricas_modelo(y_test, predicciones)

# Cálculo de las predicciones según las entradas deseadas y el arbol antes generado
# Ejemplo de clasificación
ejemplo = X_test[0] #colocar ejemplo deseado
resultado = clasificar_ejemplo(ejemplo, arbol)
print("Resultado de clasificación:", resultado)